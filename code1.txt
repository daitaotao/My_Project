import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._

object FindDifferentColumnsUDAF extends UserDefinedAggregateFunction {

  // Define input schema for the UDAF
  override def inputSchema: StructType = StructType(
    StructField("key", StringType) ::
    StructField("cols", ArrayType(StringType)) ::
    Nil
  )

  // Define buffer schema for intermediate computations
  override def bufferSchema: StructType = StructType(
    StructField("diffCols", StringType) ::
    Nil
  )

  // Define output schema of the UDAF
  override def dataType: DataType = StringType

  // Define whether the UDAF is deterministic or not
  override def deterministic: Boolean = true

  // Initialize the buffer
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer.update(0, "")
  }

  // Update the buffer with new input
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    val key = input.getString(0)
    val cols = input.getSeq[String](1)

    val distinctCols = cols.distinct
    if (distinctCols.size > 1) {
      val currentDiffCols = buffer.getString(0)
      val newDiffCols = (distinctCols diff currentDiffCols.split(",")).mkString(",")
      buffer.update(0, newDiffCols)
    }
  }

  // Merge intermediate buffers
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    val currentDiffCols = buffer1.getString(0)
    val newDiffCols = buffer2.getString(0)
    val mergedDiffCols = (currentDiffCols.split(",") ++ newDiffCols.split(",")).distinct.mkString(",")
    buffer1.update(0, mergedDiffCols)
  }

  // Evaluate the result
  override def evaluate(buffer: Row): Any = {
    buffer.getString(0)
  }
}

object Main extends App {
  val spark = SparkSession.builder()
    .appName("FindDifferentColumnsUDAFExample")
    .config("spark.master", "local")
    .getOrCreate()

  spark.udf.register("find_different_columns", FindDifferentColumnsUDAF)

  val df = spark.read.option("header", "true").csv("/path/to/your/data/file.csv")

  df.createOrReplaceTempView("your_table")

  val result = spark.sql("""
    SELECT
      key,
      find_different_columns(key, array(col1, col2, col3, ...)) AS diff_cols
    FROM your_table
    GROUP BY key
  """)

  result.show()

  spark.stop()
}
